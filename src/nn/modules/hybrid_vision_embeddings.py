import torch
import torch.nn as nn
import torch.nn.functional as F

class HybridDINOEmbedding(nn.Module):
    """
    Concatenate learned color embeddings with DINOv2 visual features.
    DINOv2 might be better for geometric patterns.
    """
    
    def __init__(self, 
                 num_colors: int = 11,
                 hidden_size: int = 512,
                 dino_model_name: str = "facebook/dinov2-base",
                 freeze_dino: bool = True):
        super().__init__()
        
        # Learned color embedding
        self.color_embedding = nn.Embedding(num_colors, hidden_size // 2)
        
        # DINOv2 vision encoder
        from transformers import AutoModel, AutoImageProcessor
        self.dino = AutoModel.from_pretrained(dino_model_name)
        self.image_processor = AutoImageProcessor.from_pretrained(dino_model_name)
        self.dino_hidden_size = self.dino.config.hidden_size  # 768 for base
        
        # Get image size from processor (handle different formats)
        if hasattr(self.image_processor, 'size'):
            if isinstance(self.image_processor.size, dict):
                # Could be {'height': 224, 'width': 224} or {'shortest_edge': 224}
                if 'height' in self.image_processor.size:
                    self.image_size = self.image_processor.size['height']
                elif 'shortest_edge' in self.image_processor.size:
                    self.image_size = self.image_processor.size['shortest_edge']
                else:
                    self.image_size = 224  # Default
            else:
                self.image_size = self.image_processor.size
        else:
            self.image_size = 224  # Default for DINOv2
        
        # Calculate patch grid size
        self.patch_size = self.dino.config.patch_size  # 14 for dinov2-base
        self.num_patches_per_side = self.image_size // self.patch_size  # 224/14 = 16
        self.num_patches = self.num_patches_per_side ** 2  # 256
        
        if freeze_dino:
            for param in self.dino.parameters():
                param.requires_grad = False
        
        # Project DINOv2 features
        self.dino_projection = nn.Linear(self.dino_hidden_size, hidden_size // 2)
        
        # Store ARC colors
        self.register_buffer('arc_colors_rgb', self._get_arc_colors_rgb())
    
    def _get_arc_colors_rgb(self) -> torch.Tensor:
        """Get ARC color palette as RGB tensor."""
        arc_colors = [
            '#000000',  # 0: Black
            '#0074D9',  # 1: Blue
            '#FF4136',  # 2: Red
            '#2ECC40',  # 3: Green
            '#FFDC00',  # 4: Yellow
            '#AAAAAA',  # 5: Gray
            '#F012BE',  # 6: Magenta
            '#FF851B',  # 7: Orange
            '#7FDBFF',  # 8: Sky blue
            '#870C25',  # 9: Maroon
            '#FFFFFF',  # 10: Padding (white)
        ]
        
        rgb_values = []
        for hex_color in arc_colors:
            hex_color = hex_color.lstrip('#')
            r = int(hex_color[0:2], 16) / 255.0
            g = int(hex_color[2:4], 16) / 255.0
            b = int(hex_color[4:6], 16) / 255.0
            rgb_values.append([r, g, b])
        
        return torch.tensor(rgb_values, dtype=torch.float32)
    
    def grid_to_rgb_image(self, x: torch.Tensor) -> torch.Tensor:
        """
        Convert integer grid to RGB image.
        
        Args:
            x: [batch, height, width] - integer values 0-10
        Returns:
            [batch, 3, image_size, image_size] - RGB image
        """
        batch, height, width = x.shape
        device = x.device
        
        # Map each integer to RGB
        rgb_image = self.arc_colors_rgb.to(device)[x]  # [B, H, W, 3]
        
        # Permute to [B, 3, H, W]
        rgb_image = rgb_image.permute(0, 3, 1, 2)
        
        # Resize to DINOv2 input size
        rgb_image = F.interpolate(
            rgb_image, 
            size=(self.image_size, self.image_size), 
            mode='nearest'
        )
        
        return rgb_image
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch, height, width]
        Returns:
            [batch, height*width, hidden_size]
        """
        batch, height, width = x.shape
        seq_len = height * width
        
        # 1. Learned embeddings
        color_emb = self.color_embedding(x).view(batch, seq_len, -1)
        
        # 2. DINOv2 features
        rgb_image = self.grid_to_rgb_image(x)
        
        # Use torch.no_grad() when frozen
        context = torch.no_grad() if not self.dino.training else torch.enable_grad()
        
        with context:
            dino_output = self.dino(pixel_values=rgb_image)
            # Remove CLS token: [B, num_patches+1, hidden] â†’ [B, num_patches, hidden]
            dino_patches = dino_output.last_hidden_state[:, 1:, :]
        
        # Project
        dino_features = self.dino_projection(dino_patches)  # [B, num_patches, hidden_size/2]
        
        # Reshape to spatial grid
        dino_features_spatial = dino_features.view(
            batch, 
            self.num_patches_per_side, 
            self.num_patches_per_side, 
            -1
        )  # [B, 16, 16, hidden_size/2]
        
        # Permute for interpolation: [B, hidden_size/2, 16, 16]
        dino_features_spatial = dino_features_spatial.permute(0, 3, 1, 2)
        
        # Interpolate to grid size
        dino_features_resized = F.interpolate(
            dino_features_spatial,
            size=(height, width),
            mode='bilinear',
            align_corners=False
        )  # [B, hidden_size/2, H, W]
        
        # Back to [B, H, W, hidden_size/2]
        dino_features_resized = dino_features_resized.permute(0, 2, 3, 1)
        dino_features_resized = dino_features_resized.view(batch, seq_len, -1)
        
        # 3. Concatenate
        combined = torch.cat([color_emb, dino_features_resized], dim=-1)
        
        return combined