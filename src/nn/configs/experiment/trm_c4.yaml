# @package _global_
# Connect Four self-play training experiment
defaults:
  - override /model: trm_c4
  - override /callbacks: default
  - override /trainer: gpu

tags: ["connectfour", "trm", "self-play"]

task_name: "train_c4"

# Experiment tracking
seed: 42

# Training configuration
timekeeping:
  max_epochs: 500
  batch_size: 512
  num_workers: 0
  steps_per_epoch: 100

model_tuning:
  
  learning_rate: 0.0002
  learning_rate_emb: 1e-2
  weight_decay: 1.0
  warmup_steps:  1000
  lr_min_ratio:  0.1
  num_heads: 8
  halt_exploration_prob: 0.1
  N_supervision: 16
  N_supervision_val: 16
  H_cycles: 3
  L_cycles: 3
  num_layers: 2
  puzzle_emb_dim: 64
  puzzle_emb_len: 4
  hidden_size: 64
  ffn_expansion: 4
  rope_theta: 10000
  seq_len: 42
  vocab_size: 3
  use_mlp_t: true

  #self-play settings
  enable_selfplay: false
  selfplay_games_per_iteration: 64
  selfplay_mcts_simulations: 400
  selfplay_eval_mcts_simulations: 50
  selfplay_parallel_simulations: 100
  selfplay_temperature_moves: 15
  selfplay_update_interval: 10  # Update "previous model" every N epochs
  selfplay_bootstrap_weight: 0.3  # 0 = pure outcome, 1 = pure MCTS value
  selfplay_temporal_decay: 0.95   # Decay bootstrap for later
  curriculum_data_path: "data/c4/minimax_games_5.pkl"
          
  # Evaluation parameters
  eval_minimax_depth: 2
  eval_minimax_temperature: 0.5
  eval_games_vs_minimax: 64
  eval_games_vs_random: 100
  eval_use_mcts: true
  
trainer:
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 5
  log_every_n_steps: 10
  num_sanity_val_steps: 0

# Paths
save_dir: /tmp/ml-experiments/connectfour
append_wandb_name_to_save_dir: true
pretrained_checkpoint_path: null

# Logger
logger:
  wandb:
    project: "trm-connectfour"
    tags: ${tags}

callbacks:
  model_checkpoint:
    monitor: "train/policy_accuracy"
  # ema:
  #   _target_: src.nn.callbacks.ema.EMACallback
  #   decay: 0.999
  #   use_ema_for_validation: true
  #   use_ema_for_test: true
