# @package _global_
# Connect Four self-play training experiment
defaults:
  - override /model: trm_c4
  - override /callbacks: default
  - override /trainer: gpu

tags: ["connectfour", "trm", "self-play"]

task_name: "train_c4"

# Experiment tracking
seed: 42

# Training configuration
timekeeping:
  max_epochs: 1000
  batch_size: 256
  num_workers: 0

# Model tuning parameters (for TRM base)
model_tuning:
# TRM architecture
  hidden_size:  256
  num_layers:  4
  num_heads:  4
  H_cycles: 2
  L_cycles: 2
  N_supervision: 2
  N_supervision_val: 2
  ffn_expansion: 2
  
  # Optimization
  learning_rate: 1e-2
  weight_decay: 1e-4
  warmup_steps: 0
  max_steps: 100000
  lr_min_ratio: 0.1
  
c4_tuning:
  # Loss weights
  policy_weight: 1.0
  value_weight: 1.0
  halt_weight: 0.5
  
  # Self-play
  games_per_epoch: 100
  buffer_size: 100000
  batch_size: 256
  steps_per_epoch: 100
  
  # MCTS
  mcts_simulations: 20
  mcts_c_puct: 1.0
  mcts_temperature: 1.0
  mcts_dirichlet_alpha: 0.3
  mcts_exploration_fraction: 0.25
  
  # Evaluation
  eval_games_vs_minimax: 20
  minimax_temperature: 0.5
  minimax_depth: 2

trainer:
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 1
  log_every_n_steps: 50
  num_sanity_val_steps: 0

# Paths
save_dir: /tmp/ml-experiments/connectfour
append_wandb_name_to_save_dir: true

# Logger
logger:
  wandb:
    project: "trm-connectfour"
    tags: ${tags}

callbacks:
  model_checkpoint:
    monitor: "val/policy_accuracy"
